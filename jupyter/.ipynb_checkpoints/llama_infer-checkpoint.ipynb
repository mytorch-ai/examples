{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b1ff20-d478-48ab-bf55-44e38215bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch version: 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Welcome to MyTorch!\n",
    "# This Notebook demonstrates how easy it is to use our cloud-based GPUs.\n",
    "# Notice that we are using standard PyTorch syntax, but our venv has MyTorch instead.\n",
    "# That's how easy it is to use MyTorch.\n",
    "#\n",
    "# Setup Instructions:\n",
    "# 1) Register at MyTorch.net and get your access token, then save it to ~/.mytorch\n",
    "#    > echo \"token=xxx\" > ~/.mytorch\n",
    "# 2) Set Up a Clean Virtual Environment\n",
    "#    Create and activate a new virtual environment to isolate your MyTorch installation:\n",
    "#    > python3 -m venv ~/venv_mytorch\n",
    "#    > source ~/venv_mytorch/bin/activate\n",
    "# 3) Install MyTorch and Jupyter\n",
    "#    > pip install --upgrade pip\n",
    "#    > pip install --upgrade mytorch-ai\n",
    "#    > pip install jupyterlab ipykernel jupyter\n",
    "# 4) Register the Virtual Environment as a Jupyter Kernel\n",
    "#    This makes your new environment available as a kernel named “mytorch (venv)” in Jupyter:\n",
    "#    > python -m ipykernel install --user --name mytorch_env --display-name \"mytorch (venv)\"\n",
    "# 5) Launch Jupyter Notebook\n",
    "#    > jupyter notebook\n",
    "\n",
    "import torch\n",
    "print(\"MyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26af7908-7dc6-497c-891f-5587b16a713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Client process is exiting; disconnecting from server...\n",
      "INFO - Connecting to server 10.8.2.17:50053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using GPU: Tesla P100-PCIE-16GB ***\n"
     ]
    }
   ],
   "source": [
    "# Step One: Confirm that MyTorch is working, GPU should be available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"*** Using GPU: {torch.cuda.get_device_name()} ***\")\n",
    "else:\n",
    "    print(\"*** No GPU available ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1797279-ee02-4e81-b691-b572032aad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does ~/.mytorch exist? True\n"
     ]
    }
   ],
   "source": [
    "# Step Two: Confirm that the MyTorch Token is saved in the correct place.\n",
    "import os\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "print(\"Does ~/.mytorch exist?\", os.path.exists(os.path.join(home_dir, \".mytorch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "946be5dc-346c-411b-9922-daafe15ec2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged into Hugging Face, which makes me happy!!!\n"
     ]
    }
   ],
   "source": [
    "# Step Three: Log in to Hugging Face\n",
    "import huggingface_hub\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "if huggingface_token is not None:\n",
    "    huggingface_hub.login(token=huggingface_token)\n",
    "else:\n",
    "    # make sure they have logged in\n",
    "    hf_token_path = os.path.join(os.path.expanduser('~'), '.cache/huggingface/token')\n",
    "    if os.path.exists(hf_token_path):\n",
    "        print(\"You are already logged into Hugging Face, which makes me happy!!!\")\n",
    "    else:\n",
    "        print(\"*** You must either set the environment variable HF_TOKEN or \\n\"\n",
    "              \"*** login to hugging face using the CLI command\", file=sys.stderr)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a84dee0-9686-4880-a873-14d955d579e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading AutoTokenizer for `meta-llama/Llama-3.2-3B-Instruct`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-3.2-3B-Instruct...\n",
      "/ Loading... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1742843613.662581  379947 chttp2_transport.cc:1201] ipv4:10.8.2.17:50053: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-03-25T08:13:33.66252484+13:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ...AutoTokenizer has been loaded\n",
      "INFO - Loading AutoModelForCausalLM model `meta-llama/Llama-3.2-3B-Instruct`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for meta-llama/Llama-3.2-3B-Instruct...\n",
      "                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ...model has been loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loading model onto cuda\n",
      "Model moved to device\n",
      "Prompt:  Tell me a 500 word story about a black cat named George\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Prompt: Tell me a 500 word story about a black cat named George\n",
      "\n",
      "Response:\n",
      "]\n",
      "George was a sleek and mysterious black cat with piercing green eyes that seemed to gleam in the dark. He had been a stray for as long as anyone could remember, but he never seemed to need anyone. He was a master of survival, able to scrounge up food and find shelter with ease.\n",
      "\n",
      "Despite his tough exterior, George had a soft spot for humans. He would often sneak into homes and curl up in laps, purring contentedly as he basked in the warmth and attention. But he was a cat of discerning taste, and only sought out those who were willing to provide him with the finest treats and the most comfortable blankets.\n",
      "\n",
      "One day, a young girl named Lily wandered into the neighborhood with her family. She was a shy and quiet child, with a mop of curly brown hair and a smile that could light up the darkest of rooms. As she explored the streets, she stumbled upon George lounging in a sunbeam, his eyes fixed intently on a nearby bird.\n",
      "\n",
      "Lily gasped in wonder, her eyes locked onto George's piercing green gaze. For a moment, the two simply stared at each other, a connection sparking between them like a live wire. Then, to Lily's surprise, George stood up and began to pad towards her, his tail twitching with curiosity.\n",
      "\n",
      "Lily hesitated for a moment, unsure of what to do. But something about George's gentle demeanor put her at ease, and she reached out a hand to pet him. George purred loudly, nuzzling her hand with his head, and Lily felt a sense of calm wash over her.\n",
      "\n",
      "From that moment on, George and Lily were inseparable. They spent their days exploring the neighborhood, chasing after butterflies, and curled up together in the sunbeams that streamed through the windows. George taught Lily all about the art of napping, and how to find the perfect spot to watch the birds outside.\n",
      "\n",
      "As the days turned into weeks, Lily's parents began to notice a change in their daughter. She seemed happier, more confident, and more at ease in her own skin. And whenever she looked into George's piercing green eyes, she felt a sense of peace and contentment wash over her.\n",
      "\n",
      "One evening, as the sun set over the neighborhood, Lily snuggled up beside George on the couch. She stroked his soft fur, feeling the warmth of his body radiate into\n",
      "\n",
      "Time taken: 24.45 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step Four: Load the Llama 3.2 3B Instruct model, then run your prompt:\n",
    "# Note this is one step because we do not want to use GPU memory for longer than necessary.\n",
    "import sys\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading tokenizer for {model}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model for {model}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model\n",
    ")\n",
    "print(\"Model loaded\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading model onto {device}\")\n",
    "model.to(device)\n",
    "print(\"Model moved to device\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Tell me a 500 word story about a black cat named George\"\n",
    "\n",
    "# Format the prompt according to Llama 2's chat template\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "inputs = tokenizer.encode_plus(\n",
    "    formatted_prompt,\n",
    "    add_special_tokens=True,  # Adds special tokens (e.g., [CLS], [SEP])\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    padding='max_length',  # Pad to a maximum length specified by the model or manually set\n",
    "    truncation=True,  # Truncate to a maximum length specified by the model or manually set\n",
    "    max_length=512  # Specify the maximum length\n",
    ")\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "# Move each tensor in the inputs dictionary to the correct device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"\\nGenerating response...\")\n",
    "  \n",
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=1000,  # Adjust based on desired length\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up the response by removing instruction tokens, the original prompt, and extra whitespace\n",
    "response = response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "response = response.replace(prompt, \"\").strip()\n",
    "end_time = time.time()\n",
    "\n",
    "print()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch (venv)",
   "language": "python",
   "name": "mytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
