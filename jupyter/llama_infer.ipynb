{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40b1ff20-d478-48ab-bf55-44e38215bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mytorch version: 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Welcome to MyTorch!\n",
    "# This Notebook demonstrates how easy it is to use our cloud-based GPUs.\n",
    "# Notice that we are using standard PyTorch syntax, but our venv has MyTorch instead.\n",
    "# That's how easy it is to use MyTorch.\n",
    "#\n",
    "# Setup Instructions:\n",
    "# 1) Register at MyTorch.net and get your access token, then save it to ~/.mytorch\n",
    "#    > echo \"token=xxx\" > ~/.mytorch\n",
    "# 2) Set Up a Clean Virtual Environment:\n",
    "#    Create and activate a new virtual environment to isolate your MyTorch installation:\n",
    "#    > python3 -m venv ~/venv_mytorch\n",
    "#    > source ~/venv_mytorch/bin/activate\n",
    "# 3) Install MyTorch and Jupyter Packages:\n",
    "#    Install the MyTorch wheel along with JupyterLab and IPython kernel support:\n",
    "#    > pip install https://raw.githubusercontent.com/mytorch-ai/examples/main/mytorch-0.3.0-py3-none-any.whl\n",
    "#    > pip install jupyterlab ipykernel\n",
    "# 4) Register the Virtual Environment as a Jupyter Kernel:\n",
    "#    This makes your new environment available as a kernel named “mytorch (venv)” in Jupyter:\n",
    "#    > python -m ipykernel install --user --name mytorch_env --display-name \"mytorch (venv)\"\n",
    "# 5) Launch Jupyter Notebook:\n",
    "#    > jupyter notebook\n",
    "\n",
    "import torch\n",
    "print(\"MyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26af7908-7dc6-497c-891f-5587b16a713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Using GPU: Tesla P100-PCIE-16GB ***\n"
     ]
    }
   ],
   "source": [
    "# Step One: Confirm that MyTorch is working, GPU should be available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"*** Using GPU: {torch.cuda.get_device_name()} ***\")\n",
    "else:\n",
    "    print(\"*** No GPU available ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1797279-ee02-4e81-b691-b572032aad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home directory: /home/footbag\n",
      "Does ~/.mytorch exist? True\n"
     ]
    }
   ],
   "source": [
    "# Step Two: Confirm that the MyTorch Token is saved in the correct place.\n",
    "import os\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "print(\"Home directory:\", home_dir)\n",
    "print(\"Does ~/.mytorch exist?\", os.path.exists(os.path.join(home_dir, \".mytorch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "946be5dc-346c-411b-9922-daafe15ec2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are already logged into Hugging Face, which makes me happy!!!\n"
     ]
    }
   ],
   "source": [
    "# Step Three: Log in to Hugging Face\n",
    "import huggingface_hub\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "if huggingface_token is not None:\n",
    "    huggingface_hub.login(token=huggingface_token)\n",
    "else:\n",
    "    # make sure they have logged in\n",
    "    hf_token_path = os.path.join(os.path.expanduser('~'), '.cache/huggingface/token')\n",
    "    if os.path.exists(hf_token_path):\n",
    "        print(\"You are already logged into Hugging Face, which makes me happy!!!\")\n",
    "    else:\n",
    "        print(\"*** You must either set the environment variable HF_TOKEN or \\n\"\n",
    "              \"*** login to hugging face using the CLI command\", file=sys.stderr)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a84dee0-9686-4880-a873-14d955d579e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Loading AutoTokenizer for `meta-llama/Llama-3.2-3B-Instruct`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for meta-llama/Llama-3.2-3B-Instruct...\n",
      "                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ...AutoTokenizer has been loaded\n",
      "INFO - Loading AutoModelForCausalLM model `meta-llama/Llama-3.2-3B-Instruct`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for meta-llama/Llama-3.2-3B-Instruct...\n",
      "                    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - ...model has been loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "Loading model onto cuda\n",
      "Model moved to device\n"
     ]
    }
   ],
   "source": [
    "# Step Four: Load the Llama 3.2 3B Instruct model\n",
    "import sys\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading tokenizer for {model}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model for {model}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model\n",
    ")\n",
    "print(\"Model loaded\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading model onto {device}\")\n",
    "model.to(device)\n",
    "print(\"Model moved to device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9b49931-edca-4f15-9bd0-380496d72a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:  Tell me a 500 word story about a black cat named George\n",
      "\n",
      "Generating response...\n",
      "\n",
      "Prompt: Tell me a 500 word story about a black cat named George\n",
      "\n",
      "Response:\n",
      "]\n",
      "George was a sleek and mysterious black cat with piercing green eyes that seemed to gleam in the dark. He had been a stray for as long as anyone could remember, prowling the streets of the small town of Willow Creek, always staying just out of reach of the humans who lived there.\n",
      "\n",
      "But George was more than just a stray. He was a guardian, a protector of the town's secrets and a keeper of its mysteries. He had a special connection to the town's history, and he knew its stories and legends better than anyone.\n",
      "\n",
      "As a young kitten, George had been found in a hidden alleyway by the town's founder, a kind old man named Silas. Silas had taken George in and raised him as his own, teaching him the ways of the town and its people. But as George grew older, he began to realize that he was more than just a pet - he was a key part of the town's magic.\n",
      "\n",
      "George's eyes seemed to hold a deep wisdom, and his fur was as black as the night sky. He moved with a silent ease, gliding across the rooftops and alleys of Willow Creek with a stealth that was unmatched. At night, he would prowl the streets, watching over the town and its inhabitants, making sure that all was well.\n",
      "\n",
      "But George's powers went beyond just watching over the town. He had a special connection to the town's history, and he could see into the past and the future. He knew the secrets of the town's founders, and he could communicate with the spirits of those who had come before.\n",
      "\n",
      "One night, a great storm was brewing over Willow Creek. The winds howled and the rain lashed down, threatening to destroy the town's fragile balance. George knew that the storm was not just a natural disaster, but a sign of something deeper - a sign that the town's magic was beginning to unravel.\n",
      "\n",
      "Determined to do something, George set out into the storm, using his powers to navigate the treacherous streets. He moved with a fierce determination, his eyes glowing like lanterns in the dark. As he prowled the streets, he could feel the magic of the town beginning to fade, and he knew that he had to do something to stop it.\n",
      "\n",
      "With a burst of speed, George leapt from rooftop to rooftop, using his agility and cunning to evade the storm's fury. He reached the town\n",
      "\n",
      "Time taken: 25.72 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step Five: Give the model a prompt.\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Tell me a 500 word story about a black cat named George\"\n",
    "\n",
    "# Format the prompt according to Llama 2's chat template\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "inputs = tokenizer.encode_plus(\n",
    "    formatted_prompt,\n",
    "    add_special_tokens=True,  # Adds special tokens (e.g., [CLS], [SEP])\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    padding='max_length',  # Pad to a maximum length specified by the model or manually set\n",
    "    truncation=True,  # Truncate to a maximum length specified by the model or manually set\n",
    "    max_length=512  # Specify the maximum length\n",
    ")\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "# Move each tensor in the inputs dictionary to the correct device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"\\nGenerating response...\")\n",
    "  \n",
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=1000,  # Adjust based on desired length\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up the response by removing instruction tokens, the original prompt, and extra whitespace\n",
    "response = response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "response = response.replace(prompt, \"\").strip()\n",
    "end_time = time.time()\n",
    "\n",
    "print()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch (venv)",
   "language": "python",
   "name": "mytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
