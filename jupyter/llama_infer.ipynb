{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b1ff20-d478-48ab-bf55-44e38215bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyTorch version: 0.3.0\n"
     ]
    }
   ],
   "source": [
    "# Welcome to MyTorch!\n",
    "# This Notebook demonstrates how easy it is to use our cloud-based GPUs.\n",
    "# Notice that we are using standard PyTorch syntax, but our venv has MyTorch instead.\n",
    "# That's how easy it is to use MyTorch.\n",
    "#\n",
    "# Setup Instructions:\n",
    "# 1) Register at MyTorch.net and get your access token, then save it to ~/.mytorch\n",
    "#    > echo \"token=xxx\" > ~/.mytorch\n",
    "# 2) Set Up a Clean Virtual Environment\n",
    "#    Create and activate a new virtual environment to isolate your MyTorch installation:\n",
    "#    > python3 -m venv ~/venv_mytorch\n",
    "#    > source ~/venv_mytorch/bin/activate\n",
    "# 3) Install MyTorch and Jupyter\n",
    "#    > pip install --upgrade pip\n",
    "#    > pip install --upgrade mytorch-ai\n",
    "#    > pip install jupyterlab ipykernel jupyter\n",
    "# 4) Register the Virtual Environment as a Jupyter Kernel\n",
    "#    This makes your new environment available as a kernel named “mytorch (venv)” in Jupyter:\n",
    "#    > python -m ipykernel install --user --name mytorch_env --display-name \"mytorch (venv)\"\n",
    "# 5) Launch Jupyter Notebook\n",
    "#    > jupyter notebook\n",
    "\n",
    "import torch\n",
    "print(\"MyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af7908-7dc6-497c-891f-5587b16a713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Client process is exiting; disconnecting from server...\n",
      "INFO - Connecting to server proxy.mytorch.net:50051\n"
     ]
    }
   ],
   "source": [
    "# Step One: Confirm that MyTorch is working, GPU should be available.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"*** Using GPU: {torch.cuda.get_device_name()} ***\")\n",
    "else:\n",
    "    print(\"*** No GPU available ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1797279-ee02-4e81-b691-b572032aad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Two: Confirm that the MyTorch Token is saved in the correct place.\n",
    "import os\n",
    "home_dir = os.path.expanduser(\"~\")\n",
    "print(\"Does ~/.mytorch exist?\", os.path.exists(os.path.join(home_dir, \".mytorch\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946be5dc-346c-411b-9922-daafe15ec2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Three: Log in to Hugging Face\n",
    "import huggingface_hub\n",
    "huggingface_token = os.getenv(\"HF_TOKEN\")\n",
    "if huggingface_token is not None:\n",
    "    huggingface_hub.login(token=huggingface_token)\n",
    "else:\n",
    "    # make sure they have logged in\n",
    "    hf_token_path = os.path.join(os.path.expanduser('~'), '.cache/huggingface/token')\n",
    "    if os.path.exists(hf_token_path):\n",
    "        print(\"You are already logged into Hugging Face, which makes me happy!!!\")\n",
    "    else:\n",
    "        import sys\n",
    "        print(\"*** You must either set the environment variable HF_TOKEN or \\n\"\n",
    "              \"*** login to hugging face using the CLI command\", file=sys.stderr)\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84dee0-9686-4880-a873-14d955d579e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Four: Load the Llama 3.2 3B Instruct model, then run your prompt:\n",
    "# Note this is one step because we do not want to use GPU memory for longer than necessary.\n",
    "import sys\n",
    "import time\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "print(f\"Loading tokenizer for {model}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Loading model for {model}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model\n",
    ")\n",
    "print(\"Model loaded\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Loading model onto {device}\")\n",
    "model.to(device)\n",
    "print(\"Model moved to device\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare the prompt\n",
    "prompt = \"Tell me a 500 word story about a black cat named George\"\n",
    "\n",
    "# Format the prompt according to Llama 2's chat template\n",
    "formatted_prompt = f\"[INST] {prompt} [/INST]\"\n",
    "\n",
    "inputs = tokenizer.encode_plus(\n",
    "    formatted_prompt,\n",
    "    add_special_tokens=True,  # Adds special tokens (e.g., [CLS], [SEP])\n",
    "    return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "    padding='max_length',  # Pad to a maximum length specified by the model or manually set\n",
    "    truncation=True,  # Truncate to a maximum length specified by the model or manually set\n",
    "    max_length=512  # Specify the maximum length\n",
    ")\n",
    "print(\"Prompt: \", prompt)\n",
    "\n",
    "# Move each tensor in the inputs dictionary to the correct device\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"\\nGenerating response...\")\n",
    "  \n",
    "# Generate the response\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_length=1000,  # Adjust based on desired length\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "# Decode and print the response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Clean up the response by removing instruction tokens, the original prompt, and extra whitespace\n",
    "response = response.replace(\"[INST]\", \"\").replace(\"[/INST]\", \"\").strip()\n",
    "response = response.replace(prompt, \"\").strip()\n",
    "end_time = time.time()\n",
    "\n",
    "print()\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(response)\n",
    "print()\n",
    "print(f\"Time taken: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23ee99-0b61-47d8-b6be-2ae39c3dac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step Five: Free up resources\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del inputs\n",
    "del outputs\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory freed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch (venv)",
   "language": "python",
   "name": "mytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
